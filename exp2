import numpy as np  
import pandas as pd  
import seaborn as sns  
import matplotlib.pyplot as plt  
from sklearn.model_selection import train_test_split  
from sklearn.linear_model import LogisticRegression  
from sklearn.metrics import classification_report, confusion_matrix  

# Load dataset
df = pd.read_csv('/content/Social_Network_Ads.csv')  
X = df.iloc[:, 1].values  
y = df.iloc[:, -1].values  
X = X.reshape(-1, 1)  
df.head()

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)  

# Preprocessing
from sklearn.preprocessing import StandardScaler, LabelEncoder  
le = LabelEncoder()  
X_train[:, 0] = le.fit_transform(X_train[:, 0])  
X_test[:, 0] = le.transform(X_test[:, 0])  
sc = StandardScaler()  
X_train = sc.fit_transform(X_train)  
X_test = sc.transform(X_test)  

# Model training
classifier = LogisticRegression(random_state=0)  
classifier.fit(X_train, y_train)  

# Predictions
print(classifier.predict(sc.transform([[87000]])))  
y_pred = classifier.predict(X_test)  
print(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), 1))  

# Metrics and visualization
from sklearn.metrics import confusion_matrix, accuracy_score  
cm = confusion_matrix(y_test, y_pred)  
accuracy_score(y_test, y_pred)  
print(classification_report(y_test, y_pred))  

# Precision-Recall curve
from sklearn.metrics import PrecisionRecallDisplay, precision_recall_curve  
precision, recall, _ = precision_recall_curve(classifier.predict(X_test), y_test)  
disp = PrecisionRecallDisplay(precision=precision, recall=recall)  
disp.plot()  
plt.title('Precision-Recall curve for Logistic Regression')  
plt.show()  

# ROC curve
from sklearn.metrics import roc_curve  
pred_prob1 = classifier.predict_proba(X_test)  
fpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob1[:, 1], pos_label=1)  
random_probs = [0 for i in range(len(y_test))]  
p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)  
plt.plot(fpr1, tpr1, linestyle='--', color='orange', label='Logistic Regression')  
plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')  
plt.xlabel('False Positive Rate')  
plt.legend(loc='best')  
plt.savefig('ROC', dpi=300)  
plt.show()

# Additional dataset
DF = pd.read_csv('/content/diabetes.csv')  
print(DF.head())  

# Handling null values
null_values = DF.isnull().sum()  
print("Null values in each column:\n", null_values)  
if DF.isnull().values.any():  
    print("\nThe DataFrame contains null values.")  
else:  
    print("\nThe DataFrame does not contain null values.")  

# Model training with additional dataset
X = DF.drop('Outcome', axis=1)  
y = DF['Outcome']  
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  
model = LogisticRegression(max_iter=1000)  
model.fit(X_train, y_train)  
y_pred = model.predict(X_test)  

# Confusion matrix for additional dataset
cm = confusion_matrix(y_test, y_pred)  
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")  
plt.xlabel("Predicted")  
plt.ylabel("Actual")  
plt.title("Confusion Matrix")  
plt.show()  

# ROC Curve for additional dataset
from sklearn.metrics import roc_curve, auc  
fpr, tpr, thresholds = roc_curve(y_test, y_pred)  
roc_auc = auc(fpr, tpr)  
plt.figure()  
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)  
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  
plt.xlabel('False Positive Rate')  
plt.ylabel('True Positive Rate')  
plt.title('Receiver Operating Characteristic (ROC) Curve')  
plt.legend(loc="lower right")  
plt.show()

# Softmax Regression
softmax_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10)  
softmax_reg.fit(X_train, y_train)  
softmax_reg.predict(sc.transform([[30, 87000, 1]]))  
softmax_reg.predict_proba(sc.transform([[30, 87000, 0]]))  

# Decision boundaries visualization
from sklearn.datasets import make_classification  
X, y = make_classification(n_samples=200, n_features=2, n_informative=2, n_redundant=0, n_classes=2, random_state=1)  
from mlxtend.plotting import plot_decision_regions  
clf = LogisticRegression()  
clf.fit(X, y)  
plot_decision_regions(X=X, y=y, clf=clf, legend=2)  
plt.title('Logistic Regression Decision Boundary')  
plt.show()
