import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.svm import SVR
from sklearn.linear_model import Ridge, Lasso
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score
from sklearn import metrics
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingRegressor, RandomForestRegressor, StackingRegressor
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import fetch_california_housing
import warnings
import xgboost as xgb
from xgboost import XGBRegressor

# Suppress warnings
warnings.filterwarnings('ignore')

# Load the California housing dataset
housing = fetch_california_housing()
X = housing.data
y = housing.target

# Load the Indian Liver Patient dataset
df = pd.read_csv('/content/indian_liver_patient.csv')
df.head()

# Check and drop missing values
df1 = df.dropna()
assert not df1.isnull().any().any(), "There are still missing values."

# Visualize correlation matrix
fig, ax = plt.subplots(figsize=(7, 7))
numerical_df = df1.select_dtypes(include=['number'])
sns.heatmap(abs(numerical_df.corr()), annot=True, square=True, cbar=False, ax=ax, linewidths=0.25)
plt.show()

# Drop correlated features
df2 = df1.drop(columns=['Direct_Bilirubin', 'Alamine_Aminotransferase', 'Total_Protiens'])

# Correct Dataset column values
df2['Dataset'] = df2['Dataset'].replace({1: 0, 2: 1})

# Analyze data
print('How many people have disease:', '\n', df2.groupby('Gender')[['Dataset']].sum(), '\n')
print('How many people participated in the study:', '\n', df2.groupby('Gender')[['Dataset']].count())
print('Percentage of people with the disease depending on gender:')
print(df2.groupby('Gender')[['Dataset']].sum() / df2.groupby('Gender')[['Dataset']].count())

# Define X and y variables
X = df2[['Gender', 'Total_Bilirubin', 'Alkaline_Phosphotase', 'Aspartate_Aminotransferase', 'Albumin', 'Albumin_and_Globulin_Ratio']]
y = pd.Series(df2['Dataset'])

# Encode categorical variable
labelencoder = LabelEncoder()
X['Gender'] = labelencoder.fit_transform(X['Gender'])

# Split data
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale data
scaler = StandardScaler()
X_train = scaler.fit_transform(x_train)
X_test = scaler.transform(x_test)

# Initialize and train AdaBoost Classifier
ADB = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=125, learning_rate=0.6, random_state=42)
ADB.fit(X_train, y_train)

# Cross-validation
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(ADB, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
print('Accuracy: %.3f' % (np.mean(n_scores) * 100))

# Evaluate and visualize confusion matrix
labels = ADB.predict(X_test)
matrix = metrics.confusion_matrix(y_test, labels)
sns.heatmap(matrix.T, square=True, annot=True, fmt='d', cbar=False)
plt.xlabel('true label')
plt.ylabel('predicted label')
plt.show()

# ROC curve
logit_roc_auc = metrics.roc_auc_score(y_test, labels)
fpr, tpr, thresholds = metrics.roc_curve(y_test, ADB.predict_proba(X_test)[:, 1])
plt.figure()
plt.plot(fpr, tpr, label='(area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1], 'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

# Load California housing dataset again
print(housing.keys())
print("Shape of dataset:", housing.data.shape)

# California housing regression
data, target = housing.data, housing.target
X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=123)

# Gradient Boosting Regressor
gbr = GradientBoostingRegressor()
gbr.fit(X_train, y_train)
ypred = gbr.predict(X_test)
mse = metrics.mean_squared_error(y_test, ypred)
print("MSE (default model): %.2f" % mse)

# XGBoost Regressor
xgb_reg = XGBRegressor(objective='reg:linear', colsample_bytree=0.3, learning_rate=0.1, max_depth=5, alpha=10, n_estimators=10)
xgb_reg.fit(X_train, y_train)
y_pred = xgb_reg.predict(X_test)
mse2 = metrics.mean_squared_error(y_test, y_pred)
print("MSE (XGBoost model): %.2f" % mse2)

# Visualize predictions
x_ax = range(len(y_test))
plt.scatter(x_ax, y_test, s=5, color="blue", label="original")
plt.plot(x_ax, y_pred, lw=0.8, color="red", label="predicted")
plt.legend()
plt.show()

