import numpy as np
import pandas as pd
import seaborn as sns
sns.set_style('whitegrid')
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
from sklearn.tree import DecisionTreeClassifier
from catboost import CatBoostClassifier, Pool, metrics, cv
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier, log_evaluation
import warnings
warnings.filterwarnings('ignore')

# Load dataset
data = pd.read_csv("/content/LoanStats3a.csv")
data.shape
pd.set_option('display.max_columns', None)
data.head(5)
data.info()

# Missing values analysis
data.isnull().sum()
pct = (data.isnull().sum().sum()) / (data.shape[0] * data.shape[1])
print("Overall missing values in the data â‰ˆ {:.2f} %".format(pct * 100))
plt.figure(figsize=(16, 14))
sns.heatmap(data.isnull())
plt.title('Null values heat plot', fontdict={'fontsize': 20})
plt.show()

# Analyze null value percentage ranges
null_percentages = (data.isnull().sum() / len(data))
ten_percent = len(null_percentages[null_percentages <= 0.1])
ten_to_twenty_percent = len(null_percentages[(null_percentages <= 0.2) & (null_percentages > 0.1)])
twenty_to_thirty_percent = len(null_percentages[(null_percentages <= 0.3) & (null_percentages > 0.2)])
thirty_to_forty_percent = len(null_percentages[(null_percentages <= 0.4) & (null_percentages > 0.3)])
forty_to_fifty_percent = len(null_percentages[(null_percentages <= 0.5) & (null_percentages > 0.4)])
fifty_to_sixty_percent = len(null_percentages[(null_percentages <= 0.6) & (null_percentages > 0.5)])
sixty_to_seventy_percent = len(null_percentages[(null_percentages <= 0.7) & (null_percentages > 0.6)])
seventy_to_eighty_percent = len(null_percentages[(null_percentages <= 0.8) & (null_percentages > 0.7)])
eighty_to_ninety_percent = len(null_percentages[(null_percentages <= 0.9) & (null_percentages > 0.8)])
hundred_percent = len(null_percentages[null_percentages > 0.9])

# Filter columns with <40% null values
df1 = data[data.columns[((data.isnull().sum()) / len(data)) < 0.4]]

# Drop constant columns
const_cols = [i for i in df1.columns if df1[i].nunique() == 1]
print("Shape before:", df1.shape)
df1.drop(const_cols, axis=1, inplace=True)
print("Shape after:", df1.shape)

# Date columns processing
dt_cols = ['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d']
for i in dt_cols:
    df1[i] = pd.to_datetime(df1[i].astype('str'), format='%b-%y', yearfirst=False)

df1['earliest_cr_line'] = pd.DatetimeIndex(df1['earliest_cr_line']).year
df1['issue_d_year'] = pd.DatetimeIndex(df1['issue_d']).year
df1['issue_d_month'] = pd.DatetimeIndex(df1['issue_d']).month
df1['last_pymnt_d_year'] = pd.DatetimeIndex(df1['last_pymnt_d']).year
df1['last_pymnt_d_month'] = pd.DatetimeIndex(df1['last_pymnt_d']).month
df1['last_credit_pull_d_year'] = pd.DatetimeIndex(df1['last_credit_pull_d']).year
df1['last_credit_pull_d_month'] = pd.DatetimeIndex(df1['last_credit_pull_d']).month

# Feature extraction and cleanup
df1['earliest_cr_line'] = 2019 - df1['earliest_cr_line']
df1['issue_d_year'] = 2019 - df1['issue_d_year']
df1['last_pymnt_d_year'] = 2019 - df1['last_pymnt_d_year']
df1['last_credit_pull_d_year'] = 2019 - df1['last_credit_pull_d_year']
df1.drop(['issue_d', 'last_pymnt_d', 'last_credit_pull_d'], axis=1, inplace=True)

# Handle missing values
df1['last_pymnt_d_year'].fillna(df1['last_pymnt_d_year'].median(), inplace=True)
df1['last_pymnt_d_month'].fillna(df1['last_pymnt_d_month'].median(), inplace=True)
df1['last_credit_pull_d_year'].fillna(df1['last_credit_pull_d_year'].median(), inplace=True)
df1['last_credit_pull_d_month'].fillna(df1['last_credit_pull_d_month'].median(), inplace=True)
df1['tax_liens'].fillna(df1['tax_liens'].median(), inplace=True)
df1['revol_util'].fillna('50%', inplace=True)
df1['revol_util'] = df1['revol_util'].str.rstrip('%').astype('float')

# Encoding categorical features
df2 = pd.get_dummies(df1, columns=['home_ownership', 'verification_status', 'purpose', 'addr_state', 'debt_settlement_flag'], drop_first=True)
le = LabelEncoder()
df2['grade'] = le.fit_transform(df2['grade'])
df2['sub_grade'] = le.fit_transform(df2['sub_grade'])

# Splitting data
X = df2.drop('loan_status', axis=1)
y = LabelEncoder().fit_transform(df2['loan_status'])
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=2)

# Model training and evaluation
models = {
    "Decision Tree": DecisionTreeClassifier(criterion='gini', random_state=100, max_depth=3, class_weight='balanced', min_samples_leaf=5),
    "CatBoost": CatBoostClassifier(iterations=5, learning_rate=0.1, verbose=False),
    "XGBoost": XGBClassifier(learning_rate=0.1, eval_metric='logloss'),
    "LightGBM": LGBMClassifier(learning_rate=0.1)
}

for name, model in models.items():
    model.fit(x_train, y_train)
    y_pred = model.predict(x_test)
    print(f"{name} Accuracy: {accuracy_score(y_test, y_pred)}")
    print(f"Classification Report for {name}:\n{classification_report(y_test, y_pred)}")

