import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# Importing sklearn Libraries
from sklearn import datasets
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.linear_model import LinearRegression
from sklearn import linear_model
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

X = np.arange(1, 25).reshape(12, 2)
y = np.array([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=4, random_state=4)

rng = np.random.RandomState(1)
x = 10 * rng.rand(50)
y = 2 * x - 5 + rng.randn(50)
plt.scatter(x, y, c='b')

model = LinearRegression(fit_intercept=True)
model.fit(x[:, np.newaxis], y)

xfit = np.linspace(0, 10, 1000)
yfit = model.predict(xfit[:, np.newaxis])
plt.scatter(x, y, c='b')
plt.plot(xfit, yfit, 'k')

def PolynomialRegression(degree=2, **kwargs):
    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))

def make_data(N, err=1.0, rseed=1):
    # randomly sample the data
    rng = np.random.RandomState(rseed)
    X = rng.rand(N, 1) ** 2
    y = 10 - 1. / (X.ravel() + 0.1)
    if err > 0:
        y += err * rng.randn(N)
    return X, y

X, y = make_data(40)
fig, ax = plt.subplots(1, 2, figsize=(16, 6))
fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)

for i, degree in enumerate([2, 9]):
    N, train_lc, val_lc = learning_curve(PolynomialRegression(degree),
                                          X, y, cv=7,
                                          train_sizes=np.linspace(0.3, 1, 25))
    ax[i].plot(N, np.mean(train_lc, 1), color='blue', label='training score')
    ax[i].plot(N, np.mean(val_lc, 1), color='red', label='validation score')
    ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1],
                 color='gray', linestyle='dashed')
    ax[i].set_ylim(0, 1)
    ax[i].set_xlim(N[0], N[-1])
    ax[i].set_xlabel('training size')
    ax[i].set_ylabel('score')
    ax[i].set_title('degree = {0}'.format(degree), size=14)
    ax[i].legend(loc='best')

auto = pd.read_csv("/content/auto-mpg.csv")
print(auto.columns)
auto.info()
auto.isna().sum()

plt.style.use('ggplot')
sns.pairplot(auto)
plt.figure(figsize=(8, 8))
sns.heatmap(auto.corr(), annot=True, linewidth=0.5, center=0)
plt.show()

auto.dtypes
auto['horsepower'].unique()

auto = auto[auto['horsepower'] != '?']
auto['horsepower'] = auto['horsepower'].astype(float)

X = auto[['displacement', 'horsepower', 'acceleration', 'model-year']]
y = auto['mpg']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)

X = auto[['displacement', 'horsepower', 'acceleration', 'model-year']]
y = auto['mpg']

X.dropna(inplace=True)  # Remove rows with any missing values in X
y = y[X.index]  # Update y to keep only the corresponding rows after dropping from X

X_test = auto

from sklearn.impute import SimpleImputer
X = auto[['displacement', 'horsepower', 'acceleration', 'model-year']]
y = auto['mpg']

imputer = SimpleImputer(strategy='mean')  # Replace NaN with the mean of the column
X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)  # Apply imputer and keep column names

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

lr = LinearRegression()
lr.fit(X_train, y_train)

pred = lr.predict(X_test)

print('Mean Absolute Error:', mean_absolute_error(y_test, pred))
print('Mean Squared Error:', mean_squared_error(y_test, pred))
print('Mean Root Squared Error:', np.sqrt(mean_squared_error(y_test, pred)))
print('Coefficient of Determination:', r2_score(y_test, pred))

pred = lr.predict(X_test)
print('Predicted fuel consumption(mpg):', pred[2])
print('Actual fuel consumption(mpg):', y_test.values[2])

df = pd.read_csv('/content/Real estate.csv')

# Dropping columns
# YOUR CODE HERE

X = df[['X2 house age']]  # Predictor variable
y = df['Y house price of unit area']  # Target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

plt.figure(figsize=(8, 6))
plt.scatter(X_test, y_test, color='blue', label='Actual')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Predicted')
plt.xlabel('House Age')
plt.ylabel('House Price per Unit Area')
plt.title('Actual vs. Predicted House Prices')
plt.legend()
plt.grid(True)
plt.show()

# Check for null values
null_values = df.isnull().sum()
print("Null values in each column:")
print(null_values)

if df.isnull().values.any():
    print("\nThere are null values in the dataframe.")
else:
    print("\nThere are no null values in the dataframe.")

plt.figure(figsize=(8, 6))
plt.scatter(df['X2 house age'], df['Y house price of unit area'])
plt.xlabel('House Age')
plt.ylabel('House Price per Unit Area')
plt.title('Scatter Plot of House Age vs. House Price')
plt.grid(True)

# Separating the data into independent and dependent variables
# YOUR CODE HERE

X = df[['X2 house age']]  # Predictor variable
y = df['Y house price of unit area']  # Target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

plt.figure(figsize=(8, 6))
plt.scatter(X_test, y_test, color='blue', label='Actual')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Predicted')
plt.xlabel('House Age')
plt.ylabel('House Price per Unit Area')
plt.title('Actual vs. Predicted House Prices')
plt.legend()
plt.grid(True)
plt.show()

n_samples, n_features = 15, 10
rng = np.random.RandomState(0)
y = rng.randn(n_samples)
X = rng.randn(n_samples, n_features)

rdg = linear_model.Ridge(alpha=0.5)  # instantiate Ridge regressor
rdg.fit(X, y)
print(rdg.score(X, y))

Lreg = linear_model.Lasso(alpha=0.5)
Lreg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
Lreg.predict([[0, 1]])

# Weight vectors
print(Lreg.coef_)
print(Lreg.intercept_)
print(Lreg.n_iter_)

ENreg = linear_model.ElasticNet(alpha=0.5, random_state=0)
ENreg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
ENreg.predict([[0, 1]])

# Weight vectors
print(ENreg.coef_)
print(ENreg.intercept_)
print(ENreg.n_iter_)

Lreg = linear_model.Lasso(alpha=0.25)
Lreg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
print(Lreg.coef_)  # Weight vectors

Lreg = linear_model.Lasso(alpha=0.5)
Lreg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
print(Lreg.coef_)  # Weight vectors

Lreg = linear_model.Lasso(alpha=0.75)
Lreg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
print(Lreg.coef_)  # Weight vectors


