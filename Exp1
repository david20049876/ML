import numpy as np 
import pandas as pd 
import seaborn as sns 
import matplotlib.pyplot as plt 
# Importing sklearn Libraries 
from sklearn import datasets 
from sklearn.preprocessing import PolynomialFeatures 
from sklearn.pipeline import make_pipeline 
from sklearn.model_selection import train_test_split, learning_curve 
from sklearn.linear_model import LinearRegression 
from sklearn import linear_model 
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score 

X = np.arange(1, 25).reshape(12, 2) 
y = np.array([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0]) 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=4, random_state=4) 

rng = np.random.RandomState(1)  
x = 10 * rng.rand(50)  
y = 2 * x - 5 + rng.randn(50) 
plt.scatter(x, y, c='b'); 

model = LinearRegression(fit_intercept=True)  
model.fit(x[:, np.newaxis], y) 
xfit = np.linspace(0, 10, 1000) 
yfit = model.predict(xfit[:, np.newaxis]) 
plt.scatter(x, y, c='b') 
plt.plot(xfit, yfit, 'k'); 

def PolynomialRegression(degree=2, **kwargs): 
    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs)) 

def make_data(N, err=1.0, rseed=1): 
    rng = np.random.RandomState(rseed) 
    X = rng.rand(N, 1) ** 2 
    y = 10 - 1. / (X.ravel() + 0.1) 
    if err > 0: 
        y += err * rng.randn(N) 
    return X, y 

X, y = make_data(40) 
fig, ax = plt.subplots(1, 2, figsize=(16, 6)) 
fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1) 
for i, degree in enumerate([2, 9]): 
    N, train_lc, val_lc = learning_curve(PolynomialRegression(degree), X, y, cv=7, 
                                         train_sizes=np.linspace(0.3, 1, 25)) 
    ax[i].plot(N, np.mean(train_lc, 1), color='blue', label='training score') 
    ax[i].plot(N, np.mean(val_lc, 1), color='red', label='validation score') 
    ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1], color='gray', linestyle='dashed') 
    ax[i].set_ylim(0, 1) 
    ax[i].set_xlim(N[0], N[-1]) 
    ax[i].set_xlabel('training size') 
    ax[i].set_ylabel('score') 
    ax[i].set_title(f'degree = {degree}', size=14) 
    ax[i].legend(loc='best')

auto = pd.read_csv("/content/auto-mpg.csv") 
print(auto.columns) 
auto.info() 
auto.isna().sum() 
plt.style.use('ggplot') 
sns.pairplot(auto) 
plt.figure(figsize=(8, 8)) 
sns.heatmap(auto.corr(), annot=True, linewidth=0.5, center=0) 
plt.show() 

auto = auto[auto['horsepower'] != '?'] 
auto['horsepower'] = auto['horsepower'].astype(float) 
X = auto[['displacement', 'horsepower', 'acceleration', 'model-year']] 
y = auto['mpg'] 

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101) 

imputer = SimpleImputer(strategy='mean') 
X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns) 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) 

lr = LinearRegression() 
lr.fit(X_train, y_train) 
pred = lr.predict(X_test) 

print('Mean Absolute Error:', mean_absolute_error(y_test, pred)) 
print('Mean Squared Error:', mean_squared_error(y_test, pred)) 
print('Mean Root Squared Error:', np.sqrt(mean_squared_error(y_test, pred))) 
print('Coefficient of Determination:', r2_score(y_test, pred)) 

df = pd.read_csv('/content/Real estate.csv') 
X = df[['X2 house age']] 
y = df['Y house price of unit area'] 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) 

model = LinearRegression() 
model.fit(X_train, y_train) 
y_pred = model.predict(X_test) 
print(f"Mean Squared Error: {mean_squared_error(y_test, y_pred)}") 
print(f"R-squared: {r2_score(y_test, y_pred)}") 
